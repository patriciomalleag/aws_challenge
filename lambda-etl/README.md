# Lambda ETL - Procesamiento Serverless

## Prop√≥sito

Este m√≥dulo implementa una funci√≥n Lambda que procesa autom√°ticamente archivos CSV cargados en S3, los valida contra esquemas JSON, los convierte a formato Parquet optimizado y cataloga los metadatos en DynamoDB.

## Arquitectura

```
S3 Event (ObjectCreated) ‚Üí Lambda ETL ‚Üí Validaci√≥n ‚Üí Conversi√≥n Parquet ‚Üí S3 Curated ‚Üí DynamoDB Catalog
```

## Funcionalidades Principales

### üîç Validaci√≥n de Esquemas
- Verificaci√≥n din√°mica contra esquemas JSON
- Detecci√≥n de columnas faltantes o adicionales
- Validaci√≥n de tipos de datos (string, number, date, boolean)
- Manejo de valores requeridos vs opcionales

### üîÑ Transformaci√≥n ETL
- Descarga temporal del CSV en `/tmp`
- **Conversi√≥n real a Parquet** usando la librer√≠a `parquetjs`
- Compresi√≥n Snappy optimizada para velocidad y tama√±o
- Procesamiento por chunks para archivos grandes
- Limpieza autom√°tica de archivos temporales
- **Validaci√≥n de integridad** de datos durante la conversi√≥n

### üìä Catalogaci√≥n
- Registro de metadatos en DynamoDB
- UUID √∫nico por dataset
- Informaci√≥n de esquema serializada
- Ruta S3 del archivo Parquet
- Timestamp de procesamiento
- Versi√≥n del dataset

### üóúÔ∏è Conversi√≥n a Parquet
- **Conversi√≥n real** usando `parquetjs` (no simulada)
- **Tipos de datos soportados**: string, integer, float, boolean, date
- **Compresi√≥n Snappy** para optimizar velocidad y tama√±o
- **Validaci√≥n de esquema** antes de la conversi√≥n
- **Estad√≠sticas de datos** calculadas autom√°ticamente
- **Progreso de conversi√≥n** monitoreado en tiempo real
- **Lectura de archivos Parquet** para verificaci√≥n

## Estructura del Proyecto

```
lambda-etl/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ index.js              # Punto de entrada principal
‚îÇ   ‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ s3EventHandler.js # Manejador de eventos S3
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ csvProcessor.js   # Procesamiento de CSV
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parquetConverter.js # Conversi√≥n a Parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ catalogService.js # Servicio de catalogaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ validators/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemaValidator.js # Validaci√≥n de esquemas
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ fileUtils.js      # Utilidades de archivos
‚îÇ       ‚îî‚îÄ‚îÄ s3Utils.js        # Utilidades de S3
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ serverless.yml           # Configuraci√≥n serverless
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ unit/
    ‚îî‚îÄ‚îÄ integration/
```

## Pruebas

### Prueba de Conversi√≥n a Parquet

Para verificar que la conversi√≥n a Parquet funciona correctamente:

```bash
# Instalar dependencias
npm install

# Ejecutar prueba de conversi√≥n a Parquet
npm run test-parquet
```

Esta prueba:
- Convierte datos de ejemplo a formato Parquet real
- Verifica la integridad de los datos
- Muestra estad√≠sticas de compresi√≥n
- Valida la lectura del archivo Parquet

## Configuraci√≥n

### Variables de Entorno

```bash
# Buckets S3
S3_BUCKET_RAW=your-raw-bucket-name
S3_BUCKET_CURATED=your-curated-bucket-name

# DynamoDB
DDB_TABLE_NAME=your-datasets-table

# Configuraci√≥n de procesamiento
MAX_FILE_SIZE_MB=100
CHUNK_SIZE_ROWS=10000
PARQUET_COMPRESSION=snappy

# Configuraci√≥n de memoria y timeout
LAMBDA_MEMORY_SIZE=1024
LAMBDA_TIMEOUT=300
```

### Esquemas de Validaci√≥n

Los esquemas se definen en formato JSON y se almacenan en S3 o se pasan como par√°metros:

```json
{
  "datasetType": "sales_data",
  "version": "1.0",
  "columns": [
    {
      "name": "id",
      "type": "number",
      "required": true,
      "description": "Identificador √∫nico"
    },
    {
      "name": "product_name",
      "type": "string",
      "required": true,
      "maxLength": 255
    },
    {
      "name": "price",
      "type": "number",
      "required": true,
      "min": 0
    },
    {
      "name": "sale_date",
      "type": "date",
      "required": true,
      "format": "YYYY-MM-DD"
    }
  ]
}
```

## Flujo de Procesamiento

### 1. Trigger del Evento
```javascript
// Evento S3 que dispara la funci√≥n
{
  "Records": [{
    "eventSource": "aws:s3",
    "eventName": "ObjectCreated:Put",
    "s3": {
      "bucket": { "name": "raw-bucket" },
      "object": { "key": "data/sales_2024.csv" }
    }
  }]
}
```

### 2. Validaci√≥n de Archivo
- Verificaci√≥n de tipo MIME (text/csv)
- Validaci√≥n de tama√±o m√°ximo
- Descarga temporal del archivo

### 3. Procesamiento CSV
- Lectura l√≠nea por l√≠nea para archivos grandes
- Validaci√≥n contra esquema JSON
- Detecci√≥n de errores de formato

### 4. Conversi√≥n Parquet
- Uso de librer√≠a `pyarrow` o `duckdb`
- Compresi√≥n Snappy para optimizaci√≥n
- Particionamiento por columnas clave

### 5. Catalogaci√≥n
- Generaci√≥n de UUID √∫nico
- Registro en DynamoDB con metadatos
- Actualizaci√≥n de versiones existentes

## Uso

### Despliegue

```bash
# Instalar dependencias
npm install

# Desplegar funci√≥n
npm run deploy

# O usando serverless framework
serverless deploy
```

### Invocaci√≥n Local

```bash
# Simular evento S3
npm run test-local

# Con archivo de prueba
npm run test-local -- --file=test-data/sample.csv
```

### Testing

```bash
# Tests unitarios
npm test

# Tests de integraci√≥n
npm run test:integration

# Tests con coverage
npm run test:coverage
```

## Salidas

### Archivo Parquet
- Ubicaci√≥n: `s3://curated-bucket/datasets/{uuid}/data.parquet`
- Compresi√≥n: Snappy
- Particionamiento: Por columnas de fecha (si aplica)

### Metadatos en DynamoDB
```json
{
  "datasetId": "uuid-12345",
  "originalFileName": "sales_2024.csv",
  "originalFileSize": 1048576,
  "parquetFileSize": 524288,
  "schema": {
    "columns": [...],
    "version": "1.0"
  },
  "processingStatus": "completed",
  "processingTimestamp": "2024-01-15T10:30:00Z",
  "rowCount": 10000,
  "columnCount": 5,
  "s3Location": {
    "bucket": "curated-bucket",
    "key": "datasets/uuid-12345/data.parquet"
  },
  "version": 1,
  "createdAt": "2024-01-15T10:30:00Z",
  "updatedAt": "2024-01-15T10:30:00Z"
}
```

## Manejo de Errores

### Errores de Validaci√≥n
- Archivo no es CSV v√°lido
- Esquema no coincide con datos
- Columnas faltantes o adicionales
- Tipos de datos incorrectos

### Errores de Procesamiento
- Memoria insuficiente
- Timeout de procesamiento
- Error en conversi√≥n Parquet
- Fallo en escritura S3

### Errores de Catalogaci√≥n
- Fallo en DynamoDB
- Error en generaci√≥n de UUID
- Conflicto de versiones

## Monitorizaci√≥n

### CloudWatch Logs
- Logs estructurados con contexto
- M√©tricas de rendimiento
- Trazabilidad completa del proceso

### M√©tricas Clave
- Tiempo de procesamiento
- Tama√±o de archivos procesados
- Tasa de √©xito/fallo
- Uso de memoria

### Alertas
- Errores de procesamiento
- Timeouts frecuentes
- Archivos muy grandes
- Fallos de validaci√≥n

## Optimizaciones

### Rendimiento
- Procesamiento por chunks
- Compresi√≥n optimizada
- Limpieza autom√°tica de archivos temporales
- Uso eficiente de memoria

### Escalabilidad
- Configuraci√≥n de memoria din√°mica
- Timeout ajustable
- Procesamiento paralelo para archivos grandes

### Costos
- Compresi√≥n eficiente
- Limpieza de archivos temporales
- Optimizaci√≥n de consultas DynamoDB

## Dependencias

### Principales
- `aws-sdk` - Cliente AWS
- `csv-parser` - Procesamiento CSV
- `pyarrow` - Conversi√≥n Parquet
- `uuid` - Generaci√≥n de IDs √∫nicos

### Desarrollo
- `jest` - Testing framework
- `serverless` - Framework de despliegue
- `eslint` - Linting de c√≥digo

## Pr√≥ximos Pasos

1. Implementar validaci√≥n de esquemas din√°mica
2. Agregar soporte para archivos Excel
3. Optimizar compresi√≥n Parquet
4. Implementar procesamiento paralelo
5. Agregar m√©tricas de calidad de datos 